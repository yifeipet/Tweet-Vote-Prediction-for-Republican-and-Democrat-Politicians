{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3  (Due: 10/31/2018)  Yifei Pei   W1468299\n",
    "\n",
    "COEN 281, Fall 2018  \n",
    "Professor Marwah\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this HW is to implement a Naive Bayes classifier to predict whether a tweet was posted by a Republican or Democrat politician. The training data consist of about 13K tweets collected before the 2006 US presidential elections, There are about an equal number of Republican and Democrat tweets, and the tweets belong to three republican and three democrat twitter accounts. \n",
    "\n",
    "To represent each tweet, we will use a commonly used model in natural language processing called 'bag of words' model. A bag of words representation of a document (tweet here) consists of words and their frequencies in the document. The order of words is ignored.  \n",
    "\n",
    "There four main tasks.\n",
    "1. Tokenization: Parsing and converting the tweets to tokens. [**This is already done for you**]\n",
    "2. Feature matrix construction from the training data set\n",
    "3. Learning Naive Bayes parameters, priors and likelihoods, from the feature matrix.\n",
    "4. Using the learned NB model to predict the labels of the test data set (about 4K tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This task consists of converting each tweet into a sequence of \"tokens\" that can be used as features. Tokens are essentially characters and character sequences obtained after using white space as a separator. A lot these are noise that we want to remove; some are words or other character sequences that are useful features. A python package called *NLTK* (natural language toolkit) contains several tokenizers, including one for tweets. We use that tokenizer; in addition we do the following:\n",
    "- remove stopwords. These are words that are frequently used in a language but do not carry any semantic information, e.g., the, an , a, this, is, was, etc.\n",
    "- make all tokens lower case (this is done by the tweet tokenizer)\n",
    "- removing twitter handles (again, done by the tweet tokenizer)\n",
    "- remove punctuations, http links\n",
    "\n",
    "Finally, we \"lemmatize\" the tokens. That means we convert different forms of a word to a common basic form, so that they can be recognized as the same work. E.g., vote, votes, voted would all be converted to vote; geese would be converted to goose,e tc. (There is a less sophisticated version of lemmatizer called a stemmer which just chops words to convert to the same base work; it doesn't work as well as a lemmatizer and we dont use it here.) There is a good description of the NLTK tokenizer [here](https://berkeley-stat159-f17.github.io/stat159-f17/lectures/11-strings/11-nltk..html).\n",
    "\n",
    "The output of this part is a cleaned up list of tokens for each tweet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yifeipei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yifeipei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#\n",
    "# you may need to run the following\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set has two columns - screen_name and text (which is the raw tweet)\n",
    "\n",
    "## load tweets\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "\n",
    "## screen_namee (accounts)\n",
    "#  democrat - hillary, time kaine, TheDemocrats\n",
    "# republicans - trunp, pence, GOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GOP', 'TheDemocrats', 'HillaryClinton', 'timkaine', 'mike_pence',\n",
       "       'realDonaldTrump'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['screen_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP</td>\n",
       "      <td>RT @GOPconvention: #Oregon votes today. That m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheDemocrats</td>\n",
       "      <td>RT @DWStweets: The choice for 2016 is clear: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>Trump's calling for trillion dollar tax cuts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>.@TimKaine's guiding principle: the belief tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timkaine</td>\n",
       "      <td>Glad the Senate could pass a #THUD / MilCon / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      screen_name                                               text\n",
       "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
       "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
       "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
       "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
       "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text\n",
       "count             13000                      13000\n",
       "unique                6                      12982\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!\n",
       "freq               2217                          4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "      <td>6554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text  label\n",
       "count             13000                      13000  13000\n",
       "unique                6                      12982      2\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!  False\n",
       "freq               2217                          4   6554"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add labels\n",
    "# 1 for D's\n",
    "# 0 for R's\n",
    "tweets['label'] = tweets['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True)\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 13K tweets, and each of the two classes have about an equal number of tweets.\n",
    "\n",
    "Now we will define our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "#  Input : dataframe with a column names 'text' which contains raw tweets (one per row)\n",
    "#  Output: A list of lists of tokens corrsponding to the 'text' column\n",
    "#\n",
    "def tokenize_tweets2(tweets):\n",
    "    \"\"\"Given a df with tweets in 'text' col, this function return tokens as a list of lists\"\"\"\n",
    "\n",
    "    # apply tokenize to the 'text' coolumn in the tweets df\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tokens = tweets['text'].apply(tweet_tokenizer.tokenize)\n",
    "    \n",
    "    # filter\n",
    "    misc = ['rt', '’', '…', '—', 'u', '”', 'w', '“', '...', '️', 'http', 'https']\n",
    "    to_remove = nltk.corpus.stopwords.words('English') + list(string.punctuation) + misc\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [[lemmatizer.lemmatize(token) for token in tw if token not in to_remove] for tw in tokens]      \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['#oregon', 'vote', 'today', 'mean', '62', 'day', 'https://t.co/OoH9FVb7QS'],\n",
       " ['choice',\n",
       "  '2016',\n",
       "  'clear',\n",
       "  'need',\n",
       "  'another',\n",
       "  'democrat',\n",
       "  'white',\n",
       "  'house',\n",
       "  '#demdebate',\n",
       "  '#wearedemocrats',\n",
       "  'http://t.co/0n5g0YN46f'],\n",
       " [\"trump's\",\n",
       "  'calling',\n",
       "  'trillion',\n",
       "  'dollar',\n",
       "  'tax',\n",
       "  'cut',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'time',\n",
       "  'pay',\n",
       "  'fair',\n",
       "  'share',\n",
       "  'https://t.co/y8vyESIOES'],\n",
       " ['guiding',\n",
       "  'principle',\n",
       "  'belief',\n",
       "  'make',\n",
       "  'difference',\n",
       "  'public',\n",
       "  'service',\n",
       "  'https://t.co/YopSUeMqOX'],\n",
       " ['glad',\n",
       "  'senate',\n",
       "  'could',\n",
       "  'pas',\n",
       "  '#thud',\n",
       "  'milcon',\n",
       "  'vetaffairs',\n",
       "  'approps',\n",
       "  'bill',\n",
       "  'solid',\n",
       "  'provision',\n",
       "  'virginia',\n",
       "  'https://t.co/NxIgRC3hDi'],\n",
       " ['exclusive',\n",
       "  'sits',\n",
       "  'see',\n",
       "  'sunday',\n",
       "  'morning',\n",
       "  '8:',\n",
       "  '30a',\n",
       "  'rtv',\n",
       "  '6',\n",
       "  'rtv',\n",
       "  '6',\n",
       "  'app'],\n",
       " ['chatham',\n",
       "  'town',\n",
       "  'council',\n",
       "  'congress',\n",
       "  'made',\n",
       "  'strong',\n",
       "  'mark',\n",
       "  'community',\n",
       "  'proud',\n",
       "  'work',\n",
       "  'together',\n",
       "  'behalf',\n",
       "  'va'],\n",
       " ['thank',\n",
       "  'new',\n",
       "  'orleans',\n",
       "  'louisiana',\n",
       "  '#makeamericagreatagain',\n",
       "  '#votetrump',\n",
       "  'https://t.co/tI1h9xT9GX',\n",
       "  'https://t.co/0bf7BOlWEj'],\n",
       " ['happy', '241st', 'birthday', 'thank', 'https://t.co/mXsxkfcstC'],\n",
       " ['excited',\n",
       "  'announce',\n",
       "  'today',\n",
       "  'plan',\n",
       "  'build',\n",
       "  'indiana',\n",
       "  'neuro-diagnostic',\n",
       "  'institute',\n",
       "  'partnership',\n",
       "  'https://t.co/hy4yRC…']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = tokenize_tweets2(tweets)\n",
    "print(len(all_tokens))\n",
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can still be improved, but we will go with this. \n",
    "\n",
    "Let's find the most common tokens, and we will use all tokens that at least occur 25 times as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hillary', 1159),\n",
       " ('trump', 1144),\n",
       " ('great', 749),\n",
       " ('clinton', 720),\n",
       " ('today', 709),\n",
       " ('make', 581),\n",
       " ('donald', 576),\n",
       " ('president', 564),\n",
       " ('day', 552),\n",
       " ('thank', 539),\n",
       " ('american', 512),\n",
       " ('new', 503),\n",
       " ('job', 503),\n",
       " ('u', 485),\n",
       " ('america', 480),\n",
       " ('people', 469),\n",
       " ('vote', 451),\n",
       " ('state', 442),\n",
       " ('get', 420),\n",
       " ('year', 415)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter([token for tokens in all_tokens for token in tokens])\n",
    "print(len(counts))\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = [k for k in counts.keys() if counts.get(k) > 25]\n",
    "len(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_words are our features.\n",
    "Now let's construct a feature matrix from these top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Martix Construction\n",
    "\n",
    "Problem 1 (15 points) Compute feature matrix\n",
    "\n",
    "Now we will extract the features from the training data and construct a feature matrix. The bad news is this matrix can be very large. In our case it is about 13K X 1K, or about 13M x 4 bytes ~ 52M, which will easily fit in the RAM of your laptops, but the training set could have easily been 10x or 100x the current size, and the number of features 10x in which case you would be out of luck. The good news is this matrix is likely to be very sparse. In fact, each tweet is not likely to contain more than 10-20 tokens, so even if this matrix becomes very large, we would be okay if we use a sparse representation.\n",
    "\n",
    "In a sparse representation, only the non-zero entities and their indices are saved. Scipy provides [several formats](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) for sparse matrices. In this assignment, it doesn't matter which one you use (in fact, we could have even used a dense matrix). However, since we have to sum along columns (or features), the most suitable one is [csc (or compressed sparse column) format](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.csc_matrix.html).\n",
    "\n",
    "To make it easier to estimate priors and likelihoods, we will construct two feature matrices - one for each for the two classes. For this, first we need to figure out how many data points are in each class.\n",
    "\n",
    "While setting elements of a csc_matrix you may get a 'SparseEfficiencyWarning'; you can ignore that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = len(top_words)\n",
    "\n",
    "# set this to the correct values\n",
    "nTrainR = tweets['screen_name'].str.contains('GOP|mike_pence|realDonaldTrump', regex=True).sum()  # number of R (0) training points\n",
    "nTrainD = tweets['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True).sum()  # number of D (1) training points\n",
    "\n",
    "# create sparse feature matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "rfmat = csc_matrix((nTrainR, num_feat), dtype=int)\n",
    "dfmat = csc_matrix((nTrainD, num_feat), dtype=int)\n",
    "\n",
    "#\n",
    "# populate rfmat and dfmat with the counts of the features\n",
    "# Remember: all tokens are not features\n",
    "#\n",
    "# a function that might be useful is <list>.index() \n",
    "#\n",
    "rfmatarray = rfmat.toarray()\n",
    "dfmatarray = dfmat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = tweets['screen_name'].str.contains('GOP|mike_pence|realDonaldTrump', regex=True)\n",
    "rnm = 0\n",
    "dnm = 0\n",
    "for i in range(len(tweets)):\n",
    "    if name[i]:\n",
    "        for j in range(len(top_words)):\n",
    "            rfmatarray[rnm][j] = all_tokens[i].count(top_words[j])\n",
    "        rnm = rnm + 1\n",
    "    else:\n",
    "        for j in range(len(top_words)):\n",
    "            dfmatarray[dnm][j] = all_tokens[i].count(top_words[j])\n",
    "        dnm = dnm +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Naive Bayes Model Parameters\n",
    "\n",
    "Problem 2 (5 points) compute log priors\n",
    "\n",
    "Problem 3 (30 points) compute log likelihoods using Laplace smoothing\n",
    "\n",
    "Now we can compute the model parameters, this is, the likelihoods and priors for the two classes. As we discussed in class, since the probabilities can be very small numbers, we will compute log likelihoods and log priors. Aslo use Laplace (aka add one) smoothing.\n",
    "\n",
    "To sum a matrix column, you can use something like dfmat[:,i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log prior for Republican: -0.9880640452604194\n",
      "The log prior for Democrat: -1.012035529743634\n",
      "The log likelihoods for Republican: [ -5.40247573  -4.19478436  -8.43067263  -4.2777207   -8.03474395\n",
      "  -6.04924352  -7.63420602  -6.10874453  -6.46914677  -6.54931712\n",
      "  -7.50867514  -6.63420602  -5.73608563 -12.67860014  -8.5911373\n",
      "  -8.77170954  -8.5911373   -6.33875014  -8.50867514  -7.54931712\n",
      "  -8.43067263  -5.09363764  -7.92371264  -7.82061914  -8.03474395\n",
      "  -9.35667204  -4.75378764  -8.43067263  -7.21916852  -8.77170954\n",
      "  -7.54931712  -7.87124522  -9.67860014  -6.67860014  -7.82061914\n",
      "  -5.85842118  -6.43067263  -8.67860014  -7.97816042  -8.5911373\n",
      "  -8.97816042  -6.21916852  -6.61251095  -7.77170954  -7.39319792\n",
      "  -6.06389029  -6.2692092   -6.48877558  -8.21916852  -3.87770024\n",
      "  -4.2777207   -5.02754845  -7.03474395  -6.57007568  -7.63420602\n",
      "  -7.50867514  -7.77170954  -6.02038866  -7.92371264  -4.30791273\n",
      "  -8.87124522  -5.89724043  -6.06389029  -4.8268511   -6.82061914\n",
      "  -8.03474395  -9.35667204  -4.77170954 -12.67860014  -8.50867514\n",
      "  -8.03474395  -6.5911373   -8.03474395  -9.09363764  -7.43067263\n",
      "  -8.03474395  -5.64517714  -5.35667204  -5.52885302  -6.67860014\n",
      " -12.67860014  -6.63420602  -7.5911373   -5.58056806  -6.13944133\n",
      "  -5.71281585  -4.37026111  -5.93713315  -8.97816042  -5.36571718\n",
      "  -6.57007568  -7.5911373   -9.09363764  -7.77170954  -8.5911373\n",
      "  -8.77170954  -7.97816042  -6.0786873   -6.7478628   -6.44978145\n",
      "  -8.28628272  -7.32104813  -6.46914677  -6.7478628   -6.57007568\n",
      "  -7.12401129  -6.20286671  -8.21916852  -7.67860014  -7.54931712\n",
      "  -6.37481939  -7.72440383  -9.50867514  -8.28628272  -6.57007568\n",
      "  -7.09363764  -8.03474395  -9.67860014  -7.28628272  -9.09363764\n",
      "  -8.77170954  -6.18674704  -8.50867514  -6.54931712  -7.87124522\n",
      "  -8.43067263 -11.09363764  -8.28628272  -5.49869105  -6.21916852\n",
      "  -6.02038866  -8.35667204  -7.43067263  -3.52378203  -9.09363764\n",
      "  -9.50867514  -5.13170568  -5.17875425  -6.79595709  -4.56485797\n",
      "  -5.32987198  -5.20286671  -8.15503818  -7.39319792  -5.83311009\n",
      "  -6.39319792  -6.03474395  -5.97816042  -6.43067263  -9.21916852\n",
      "  -6.67860014  -6.5911373   -7.46914677  -3.47892779  -7.43067263\n",
      "  -8.21916852  -4.37026111  -8.87124522  -8.50867514  -6.10874453\n",
      "  -7.97816042  -6.25233538  -8.28628272  -6.57007568  -6.70132022\n",
      "  -7.5911373   -6.87124522  -9.35667204  -8.28628272 -10.35667204\n",
      "  -8.97816042  -8.5911373   -7.35667204  -6.20286671  -5.39319792\n",
      "  -6.18674704  -5.6233177   -7.97816042  -7.21916852  -7.21916852\n",
      "  -8.50867514  -8.87124522  -4.87770024  -7.21916852  -6.30356071\n",
      "  -6.70132022  -7.28628272  -8.15503818  -8.35667204  -7.5911373\n",
      "  -6.89724043  -7.63420602  -3.44737896  -7.28628272  -7.5911373\n",
      "  -5.1708055   -7.82061914  -6.44978145  -7.06389029  -6.50867514\n",
      "  -5.91041581  -7.92371264  -6.82061914  -9.87124522  -6.82061914\n",
      "  -5.83311009  -8.03474395  -8.5911373   -5.88418427  -6.89724043\n",
      "  -6.87124522  -6.1708055   -9.09363764  -7.87124522  -8.5911373\n",
      "  -4.52378203  -8.28628272  -6.54931712  -6.97816042  -5.92371264\n",
      "  -7.5911373   -7.77170954  -7.97816042  -6.50867514  -6.84571012\n",
      "  -7.92371264  -7.97816042  -7.46914677  -7.43067263  -8.21916852\n",
      "  -7.72440383  -9.21916852  -7.92371264  -6.25233538  -5.60178454\n",
      "  -7.87124522  -8.35667204  -8.28628272  -6.87124522  -7.35667204\n",
      "  -7.18674704  -6.54931712  -8.35667204  -6.77170954  -7.87124522\n",
      "  -5.97816042  -8.87124522 -12.67860014  -7.63420602  -7.63420602\n",
      " -12.67860014  -7.82061914  -7.43067263  -7.72440383  -8.67860014\n",
      "  -6.50867514  -8.09363764  -9.67860014  -6.65623233  -8.28628272\n",
      "  -7.18674704  -7.54931712  -7.25233538  -5.31227792  -8.97816042\n",
      "  -8.77170954  -7.97816042  -7.39319792  -8.21916852  -8.21916852\n",
      "  -6.46914677  -8.15503818  -7.21916852  -6.84571012  -7.39319792\n",
      "  -6.87124522  -7.5911373   -8.5911373   -6.77170954  -7.54931712\n",
      "  -7.67860014  -7.63420602  -6.97816042  -7.46914677  -7.54931712\n",
      "  -4.83939635  -6.95067968  -8.5911373   -6.95067968  -8.67860014\n",
      "  -9.21916852  -8.67860014  -8.50867514  -7.46914677  -6.67860014\n",
      "  -8.03474395  -6.18674704  -7.67860014  -7.82061914 -10.09363764\n",
      "  -8.87124522  -7.18674704  -7.67860014  -8.03474395  -7.35667204\n",
      "  -6.44978145  -6.79595709  -5.84571012  -8.87124522  -8.03474395\n",
      "  -8.5911373   -6.7478628   -7.67860014  -6.54931712  -8.15503818\n",
      "  -7.21916852  -7.82061914  -7.15503818  -9.09363764  -8.50867514\n",
      "  -6.44978145  -9.09363764  -8.35667204  -7.92371264  -7.28628272\n",
      "  -6.95067968  -6.97816042  -8.21916852  -8.03474395  -7.82061914\n",
      "  -6.30356071  -5.54931712  -8.77170954  -8.77170954  -8.28628272\n",
      "  -8.50867514  -9.87124522  -7.77170954  -9.50867514  -8.28628272\n",
      "  -8.28628272  -9.09363764  -7.46914677  -6.63420602  -9.21916852\n",
      "  -7.67860014  -7.28628272  -8.87124522  -8.15503818  -8.67860014\n",
      "  -7.46914677  -7.25233538  -7.82061914  -7.18674704  -7.09363764\n",
      "  -5.77170954  -7.82061914  -7.87124522  -8.43067263  -8.43067263\n",
      "  -7.50867514  -7.12401129  -6.97816042  -7.5911373   -7.46914677\n",
      "  -7.35667204  -9.67860014  -8.15503818  -7.82061914  -7.32104813\n",
      "  -7.97816042  -7.5911373   -5.65623233  -7.46914677  -8.5911373\n",
      " -10.35667204  -7.54931712  -7.21916852  -5.40247573  -6.87124522\n",
      "  -7.09363764  -7.97816042  -8.03474395  -8.67860014  -9.09363764\n",
      "  -8.03474395  -7.06389029  -6.77170954  -7.92371264  -8.5911373\n",
      "  -9.09363764  -8.09363764  -8.50867514  -8.15503818 -11.09363764\n",
      "  -7.46914677  -8.5911373   -7.06389029  -5.93713315  -7.39319792\n",
      "  -7.97816042  -6.44978145  -8.5911373   -5.95067968  -8.97816042\n",
      "  -8.77170954  -9.21916852  -8.5911373  -11.67860014  -5.4401954\n",
      "  -8.50867514  -7.77170954  -7.21916852  -6.18674704  -6.57007568\n",
      "  -7.63420602  -7.5911373   -8.50867514  -6.50867514  -6.82061914\n",
      "  -5.83311009  -7.18674704  -7.18674704  -6.2692092   -7.12401129\n",
      "  -8.67860014  -8.28628272  -7.54931712  -9.87124522  -8.50867514\n",
      "  -6.04924352  -6.5911373   -7.18674704  -6.61251095  -7.87124522\n",
      "  -7.82061914  -7.0061748  -11.09363764 -12.67860014  -8.87124522\n",
      "  -9.87124522  -9.87124522 -12.67860014  -8.87124522  -7.50867514\n",
      "  -7.18674704  -8.21916852  -8.15503818  -8.43067263  -8.43067263\n",
      "  -8.5911373   -6.21916852  -7.87124522  -8.28628272  -8.28628272\n",
      "  -9.50867514  -5.95067968  -8.28628272  -6.18674704  -7.63420602\n",
      "  -8.50867514  -7.0061748   -7.09363764  -9.67860014  -8.09363764\n",
      "  -7.72440383  -7.09363764  -7.09363764  -9.21916852  -7.82061914\n",
      "  -9.35667204  -9.87124522  -8.03474395  -6.50867514  -5.49869105\n",
      "  -7.72440383  -7.06389029  -6.89724043  -7.63420602  -9.35667204\n",
      "  -7.09363764  -7.92371264  -8.15503818  -7.46914677  -8.43067263\n",
      "  -9.35667204  -6.7478628   -8.50867514  -7.67860014  -8.87124522\n",
      "  -8.50867514  -6.89724043  -8.21916852  -7.97816042  -8.03474395\n",
      "  -8.67860014  -7.5911373   -8.97816042  -7.12401129  -8.5911373\n",
      "  -7.72440383  -5.82061914  -7.5911373   -9.50867514 -11.09363764\n",
      "  -7.12401129  -8.67860014  -7.09363764  -7.32104813  -5.33875014\n",
      "  -5.64517714  -7.50867514  -7.43067263  -7.35667204  -7.92371264\n",
      "  -6.23565664  -7.06389029  -7.25233538  -5.49869105  -8.97816042\n",
      "  -8.15503818  -7.87124522  -6.10874453  -7.18674704  -6.72440383\n",
      "  -7.63420602  -8.43067263  -8.50867514  -7.43067263  -7.92371264\n",
      "  -9.50867514  -8.50867514 -12.67860014  -9.21916852  -8.35667204\n",
      "  -7.63420602  -8.21916852  -6.82061914  -7.97816042  -8.03474395\n",
      "  -8.97816042  -8.15503818  -9.67860014  -9.67860014  -7.87124522\n",
      "  -9.35667204  -8.15503818  -7.92371264  -6.61251095  -7.72440383\n",
      "  -9.09363764  -7.5911373   -7.67860014  -8.5911373   -8.28628272\n",
      "  -8.09363764  -7.77170954  -8.15503818  -8.28628272  -7.06389029\n",
      "  -8.28628272  -8.35667204  -8.03474395  -7.0061748   -8.03474395\n",
      "  -7.87124522  -8.87124522  -7.39319792  -7.0061748   -9.09363764\n",
      "  -7.0061748   -7.97816042  -9.09363764  -7.18674704  -6.87124522\n",
      "  -8.77170954  -8.35667204  -7.92371264  -8.09363764  -7.82061914\n",
      "  -8.35667204  -7.43067263  -9.50867514  -8.87124522  -7.87124522\n",
      "  -9.50867514  -6.97816042  -9.35667204  -8.67860014  -7.97816042\n",
      "  -8.09363764  -8.15503818  -9.50867514  -9.50867514  -8.35667204\n",
      "  -7.32104813  -9.21916852  -7.92371264  -6.39319792  -6.46914677\n",
      "  -7.92371264  -7.97816042  -7.63420602  -7.21916852  -7.77170954\n",
      "  -8.43067263  -8.43067263  -9.21916852  -7.92371264  -8.15503818\n",
      "  -7.72440383  -8.35667204  -8.87124522  -8.21916852  -7.77170954\n",
      "  -8.28628272  -7.15503818  -8.09363764  -7.25233538  -6.7478628\n",
      "  -8.67860014  -8.15503818  -7.09363764  -9.09363764  -7.46914677\n",
      "  -7.5911373   -7.35667204  -8.5911373   -7.72440383  -7.63420602\n",
      "  -7.97816042 -11.67860014  -8.5911373  -12.67860014 -12.67860014\n",
      "  -8.03474395  -7.82061914  -7.21916852  -8.50867514  -7.09363764\n",
      " -10.09363764  -6.67860014  -8.87124522  -9.35667204  -8.28628272\n",
      "  -7.35667204  -9.50867514  -6.30356071  -8.28628272  -6.95067968\n",
      "  -7.87124522  -6.87124522  -8.5911373   -7.50867514  -9.09363764\n",
      "  -9.87124522  -6.21916852  -8.50867514  -6.57007568  -8.03474395\n",
      "  -8.09363764  -8.43067263  -8.5911373   -8.09363764  -8.35667204\n",
      "  -6.13944133  -6.67860014  -6.43067263  -8.15503818  -7.87124522\n",
      "  -7.46914677  -7.72440383  -7.82061914 -12.67860014  -8.35667204\n",
      "  -7.77170954  -7.09363764  -7.12401129  -7.0061748   -7.15503818\n",
      "  -8.87124522  -7.28628272  -7.12401129  -7.67860014  -7.82061914\n",
      "  -8.87124522  -9.09363764  -7.77170954 -10.09363764  -8.28628272\n",
      "  -8.03474395  -8.77170954  -8.15503818  -8.5911373   -9.35667204\n",
      "  -6.67860014  -7.35667204  -7.5911373   -7.67860014  -8.43067263\n",
      "  -9.09363764  -8.15503818  -8.03474395  -8.43067263  -8.50867514\n",
      "  -9.50867514  -8.50867514  -8.50867514  -7.72440383  -7.12401129\n",
      "  -6.84571012  -9.50867514 -11.67860014 -10.09363764  -8.87124522\n",
      "  -8.09363764  -6.72440383  -7.35667204  -8.09363764  -8.28628272\n",
      "  -8.35667204  -8.21916852  -9.35667204  -8.87124522  -7.67860014\n",
      "  -8.28628272  -8.43067263  -8.50867514  -7.92371264  -8.35667204\n",
      "  -8.5911373   -7.97816042  -8.5911373   -7.63420602  -7.77170954\n",
      "  -8.09363764  -8.67860014  -8.50867514  -8.87124522  -7.25233538\n",
      "  -7.46914677  -8.21916852  -8.03474395  -7.0061748   -8.35667204\n",
      "  -8.50867514  -7.92371264 -12.67860014  -8.43067263  -8.50867514\n",
      "  -6.92371264  -7.97816042 -10.67860014  -8.87124522  -8.35667204\n",
      "  -7.82061914  -7.03474395  -8.28628272  -8.35667204  -9.09363764\n",
      "  -8.97816042  -9.35667204  -7.63420602  -7.63420602  -7.87124522\n",
      "  -7.97816042  -8.77170954 -10.35667204 -10.35667204  -8.28628272\n",
      "  -7.5911373   -7.92371264  -8.28628272  -7.46914677  -8.77170954\n",
      "  -7.67860014  -9.35667204  -8.09363764  -7.97816042  -6.50867514\n",
      "  -8.28628272  -7.50867514  -8.28628272  -9.21916852  -8.87124522\n",
      " -11.67860014  -8.50867514  -8.15503818  -8.5911373   -6.37481939\n",
      "  -7.97816042  -7.97816042  -7.35667204  -8.15503818  -8.87124522\n",
      "  -8.15503818 -10.09363764  -7.72440383  -7.82061914  -8.77170954\n",
      "  -8.35667204  -8.03474395 -11.67860014  -9.09363764  -7.5911373\n",
      "  -7.77170954  -7.50867514  -8.67860014  -7.97816042  -7.82061914\n",
      "  -7.92371264  -7.63420602  -7.82061914 -10.35667204  -8.35667204\n",
      "  -7.77170954  -7.67860014  -8.35667204  -8.87124522  -7.97816042\n",
      " -12.67860014  -7.87124522  -8.21916852  -8.87124522  -9.35667204\n",
      "  -8.97816042  -9.67860014  -8.87124522  -9.67860014  -8.03474395\n",
      " -11.09363764  -9.67860014  -8.35667204  -7.87124522  -8.03474395\n",
      "  -8.09363764  -7.50867514  -7.39319792  -7.39319792  -8.50867514\n",
      "  -6.84571012  -8.03474395  -8.03474395  -8.35667204  -8.28628272\n",
      "  -8.28628272  -7.72440383  -8.35667204 -10.09363764  -8.35667204\n",
      "  -9.67860014  -9.35667204  -7.72440383  -9.21916852 -12.67860014\n",
      " -12.67860014 -12.67860014 -10.67860014  -7.46914677  -9.09363764\n",
      "  -8.35667204  -9.21916852  -8.35667204  -8.97816042  -8.67860014\n",
      "  -8.43067263  -8.5911373   -9.09363764  -8.15503818  -8.03474395\n",
      "  -9.67860014  -7.82061914  -8.15503818  -8.97816042  -8.09363764\n",
      "  -7.63420602  -8.03474395  -8.09363764  -8.67860014 -11.67860014\n",
      "  -8.97816042  -8.43067263  -9.67860014 -11.67860014  -7.92371264\n",
      "  -9.50867514  -8.28628272 -12.67860014  -8.77170954  -8.5911373\n",
      " -10.09363764 -11.67860014  -8.28628272  -8.5911373  -10.09363764\n",
      "  -8.67860014  -7.5911373  -10.35667204 -10.67860014  -8.03474395\n",
      " -10.35667204  -8.15503818  -8.97816042  -8.15503818  -9.21916852\n",
      "  -8.87124522  -6.87124522  -8.50867514  -8.28628272  -7.43067263\n",
      "  -7.92371264  -9.21916852 -12.67860014  -8.50867514  -8.50867514\n",
      "  -8.87124522  -7.97816042 -10.09363764  -8.97816042  -7.92371264\n",
      "  -8.21916852  -8.87124522  -7.77170954  -7.87124522  -8.5911373\n",
      "  -8.28628272  -8.43067263]\n",
      "The log likelihoods for Democrat: [ -4.43546751  -4.19111166  -7.29708402  -4.89974853  -7.22837127\n",
      "  -6.13107407  -7.56717319  -4.77811908  -6.79665503  -4.77199298\n",
      "  -6.92671557  -6.61024191  -5.89974853  -7.84728111  -5.21169253\n",
      "  -8.26231861  -9.06967353  -5.83445707  -7.26231861  -7.36923381\n",
      "  -8.56717319  -5.23678351  -6.65463603  -8.19520441  -7.40670852\n",
      "  -8.48471103  -4.24524509  -6.84728111  -6.52535301  -8.01077984\n",
      "  -5.35085528  -6.89974853  -7.26231861  -5.54611157  -5.70043972\n",
      "  -6.01077984  -6.82174601  -8.40670852  -8.33270793  -8.48471103\n",
      "  -5.37851162  -6.13107407  -7.01077984  -7.56717319  -6.31478603\n",
      "  -5.42581734  -5.15479014  -5.77199298  -5.20342492  -6.08478042\n",
      "  -5.27093174 -12.65463603 -12.65463603  -6.16278293  -7.52535301\n",
      "  -7.01077984  -9.19520441  -5.39724819  -7.22837127  -9.48471103\n",
      "  -8.06967353  -5.49476469  -6.24524509  -4.74774543  -6.95419631\n",
      "  -8.19520441  -8.26231861  -4.73577279  -7.40670852  -8.26231861\n",
      "  -8.33270793  -6.65463603  -9.33270793  -8.40670852  -6.40670852\n",
      " -10.06967353  -5.41623129  -3.98930011  -5.34175307  -6.77199298\n",
      "  -7.19520441  -6.24524509  -7.79665503  -5.52535301  -6.21169253\n",
      "  -7.33270793  -5.10004718  -6.11547722  -7.36923381  -6.10004718\n",
      "  -6.38784949  -8.33270793  -7.70043972  -6.98221069  -7.70043972\n",
      "  -8.06967353  -6.63226822  -7.03992618  -6.44518266  -7.26231861\n",
      "  -8.74774543  -9.65463603  -9.84728111  -7.36923381  -6.74774543\n",
      "  -8.19520441  -8.56717319  -8.56717319  -7.26231861  -6.54611157\n",
      "  -5.82174601  -5.62121303  -7.70043972  -8.19520441  -5.62121303\n",
      "  -8.13107407  -6.67735611  -6.13107407  -6.79665503  -7.22837127\n",
      "  -7.03992618  -6.13107407  -7.13107407  -9.84728111 -10.65463603\n",
      "  -7.65463603  -7.10004718  -8.74774543  -5.50488891  -7.10004718\n",
      "  -5.87327632  -8.26231861  -6.50488891  -5.15479014  -8.33270793\n",
      "  -7.95419631  -5.9681355   -4.39724819  -3.6350453   -2.89142366\n",
      "  -4.68309247  -4.75376922  -6.61024191  -6.33270793  -6.92671557\n",
      "  -6.42581734  -5.65463603  -5.79665503  -7.44518266  -8.19520441\n",
      "  -6.19520441  -4.79665503  -6.02527941  -3.4922447   -7.48471103\n",
      "  -9.33270793  -5.66595134  -8.19520441  -6.08478042  -4.68309247\n",
      "  -7.79665503  -6.19520441  -7.10004718  -8.33270793  -6.84728111\n",
      "  -6.14684139  -7.79665503  -8.33270793  -7.89974853  -7.79665503\n",
      "  -8.56717319  -7.44518266  -7.89974853 -10.33270793  -9.84728111\n",
      "  -6.22837127  -5.24524509  -8.95419631  -8.06967353  -8.74774543\n",
      "  -8.01077984  -8.56717319  -5.01801141 -10.33270793  -7.89974853\n",
      "  -6.31478603  -7.33270793  -7.40670852  -8.56717319  -8.13107407\n",
      "  -8.65463603  -7.13107407  -5.73577279  -7.03992618  -9.65463603\n",
      " -12.65463603  -7.01077984  -5.91316904  -6.33270793  -7.95419631\n",
      "  -5.43546751  -6.95419631  -8.06967353  -7.22837127  -5.0923936\n",
      "  -6.33270793 -10.06967353  -8.13107407  -5.70043972  -5.55660395\n",
      "  -6.54611157  -5.91316904  -7.06967353  -8.26231861  -8.33270793\n",
      " -11.65463603  -7.84728111  -9.33270793  -5.86022016  -5.71212152\n",
      "  -8.56717319 -10.06967353 -10.65463603 -12.65463603  -6.33270793\n",
      "  -8.48471103  -8.13107407  -9.19520441  -8.74774543  -9.65463603\n",
      "  -7.06967353  -7.29708402 -12.65463603  -7.70043972  -7.48471103\n",
      "  -8.26231861  -9.65463603  -9.19520441  -7.70043972  -7.10004718\n",
      "  -7.06967353  -5.72389869  -8.95419631  -5.53569496  -6.67735611\n",
      "  -6.70043972  -6.56717319  -6.35085528  -6.13107407  -6.36923381\n",
      "  -5.73577279  -8.84728111  -8.33270793 -10.33270793  -8.40670852\n",
      "  -6.2795966   -9.06967353  -7.95419631  -7.65463603  -7.84728111\n",
      "  -7.89974853  -9.84728111  -6.48471103  -4.2075528   -7.13107407\n",
      "  -7.29708402  -7.52535301  -8.33270793  -7.84728111  -7.40670852\n",
      "  -7.16278293  -9.84728111 -10.33270793  -7.79665503  -8.95419631\n",
      "  -7.65463603 -12.65463603  -8.40670852  -6.35085528 -11.65463603\n",
      "  -9.48471103  -7.61024191 -10.33270793 -10.65463603  -9.06967353\n",
      " -12.65463603  -6.74774543  -7.44518266  -5.62121303  -7.89974853\n",
      "  -7.84728111  -8.19520441  -8.13107407 -12.65463603 -10.65463603\n",
      "  -9.65463603  -8.19520441  -8.06967353  -8.06967353  -7.74774543\n",
      "  -8.40670852  -6.16278293  -7.95419631  -9.19520441 -12.65463603\n",
      "  -6.84728111  -6.10004718  -5.91316904  -8.56717319  -7.44518266\n",
      "  -6.29708402  -5.70043972  -8.56717319 -12.65463603  -7.65463603\n",
      " -10.33270793  -7.40670852  -6.52535301  -8.40670852  -8.40670852\n",
      "  -4.96114907  -8.19520441  -8.33270793  -7.74774543  -8.65463603\n",
      "  -6.26231861  -6.82174601  -9.33270793  -7.48471103  -9.06967353\n",
      "  -6.19520441  -5.25375659  -8.01077984  -8.84728111  -7.22837127\n",
      "  -7.19520441  -7.79665503  -6.87327632  -8.40670852  -7.79665503\n",
      "  -8.56717319  -8.06967353  -7.74774543 -12.65463603  -8.19520441\n",
      "  -5.55660395  -9.06967353  -7.89974853  -7.19520441  -7.56717319\n",
      " -12.65463603  -6.87327632  -8.26231861  -8.13107407  -7.16278293\n",
      "  -5.55660395  -8.40670852 -11.65463603  -9.33270793  -9.48471103\n",
      "  -9.33270793  -6.63226822  -6.98221069  -6.77199298 -12.65463603\n",
      "  -6.65463603  -7.95419631  -6.89974853  -8.65463603  -6.42581734\n",
      " -11.65463603  -8.26231861  -5.37851162  -8.95419631  -7.03992618\n",
      "  -7.61024191  -7.74774543  -7.29708402  -5.43546751  -7.19520441\n",
      "  -9.33270793  -7.65463603  -7.84728111  -6.65463603  -8.56717319\n",
      "  -7.16278293  -7.79665503  -5.8864517   -8.06967353  -8.95419631\n",
      "  -8.65463603  -9.33270793  -8.40670852  -7.89974853  -6.82174601\n",
      "  -9.65463603  -8.19520441  -6.58854684  -7.95419631  -7.06967353\n",
      "  -7.89974853  -6.35085528  -7.95419631  -5.89974853  -8.65463603\n",
      "  -6.89974853  -7.84728111  -8.65463603  -6.92671557  -5.55660395\n",
      "  -6.77199298  -7.89974853  -8.06967353 -12.65463603  -7.29708402\n",
      "  -8.40670852  -9.84728111  -8.40670852  -8.95419631  -6.10004718\n",
      "  -6.26231861  -6.82174601  -6.82174601  -6.92671557  -9.33270793\n",
      "  -7.70043972  -9.06967353  -6.44518266  -7.70043972  -8.84728111\n",
      "  -7.56717319 -12.65463603  -7.65463603  -7.79665503  -7.84728111\n",
      "  -9.19520441  -8.48471103  -7.65463603  -7.44518266  -5.79665503\n",
      "  -7.36923381  -5.59935359  -6.42581734  -7.19520441  -8.01077984\n",
      "  -7.65463603  -9.33270793  -7.33270793  -7.95419631  -7.13107407\n",
      "  -6.77199298  -8.13107407  -9.65463603  -7.36923381  -9.84728111\n",
      "  -8.26231861  -6.42581734  -7.56717319  -7.01077984  -8.13107407\n",
      "  -8.74774543  -9.84728111  -7.65463603  -7.89974853  -9.19520441\n",
      "  -8.74774543  -7.89974853  -7.74774543  -7.79665503  -8.33270793\n",
      "  -8.26231861  -8.19520441  -8.48471103  -7.22837127  -5.86022016\n",
      " -10.33270793  -7.48471103  -6.72389869  -7.79665503  -6.79665503\n",
      "  -6.74774543  -9.65463603  -8.74774543  -7.06967353  -8.95419631\n",
      "  -8.48471103  -7.61024191  -9.06967353  -8.95419631  -8.40670852\n",
      "  -7.89974853 -11.65463603  -6.82174601  -8.56717319  -9.84728111\n",
      "  -7.95419631  -6.11547722  -7.01077984  -7.01077984  -8.95419631\n",
      "  -5.74774543  -5.30590787  -7.22837127  -8.19520441  -8.01077984\n",
      "  -7.33270793  -7.44518266  -7.56717319 -12.65463603  -7.06967353\n",
      " -12.65463603 -10.33270793  -7.89974853  -8.48471103  -8.06967353\n",
      "  -8.06967353  -8.95419631  -8.01077984  -8.56717319  -6.40670852\n",
      "  -7.84728111  -7.29708402  -5.91316904  -7.56717319  -7.33270793\n",
      "  -8.40670852  -7.19520441  -8.65463603  -8.56717319  -8.01077984\n",
      "  -8.01077984  -8.48471103  -7.89974853  -7.36923381  -7.33270793\n",
      "  -8.26231861  -8.40670852  -8.33270793  -7.74774543  -8.48471103\n",
      "  -7.06967353  -6.95419631  -7.89974853  -7.61024191  -7.95419631\n",
      "  -8.48471103  -7.84728111  -8.56717319  -9.84728111 -10.33270793\n",
      "  -8.19520441 -12.65463603 -12.65463603  -8.56717319  -8.74774543\n",
      "  -8.84728111  -7.13107407  -8.26231861  -8.06967353  -7.52535301\n",
      "  -5.87327632  -7.95419631  -8.40670852  -8.74774543 -10.65463603\n",
      "  -8.65463603  -8.33270793  -6.89974853  -7.56717319  -7.79665503\n",
      "  -6.58854684  -6.08478042  -7.79665503  -7.16278293 -12.65463603\n",
      "  -7.65463603  -7.48471103  -8.48471103  -9.33270793  -9.48471103\n",
      "  -8.84728111  -8.84728111  -6.63226822  -8.40670852  -6.79665503\n",
      "  -7.06967353  -7.84728111  -8.06967353  -7.79665503  -6.65463603\n",
      "  -8.26231861  -7.19520441  -8.01077984  -7.89974853  -9.48471103\n",
      "  -7.95419631  -8.33270793 -11.06967353  -8.65463603  -8.95419631\n",
      "  -8.48471103  -9.84728111  -7.79665503  -6.08478042  -8.13107407\n",
      "  -8.95419631  -9.19520441  -8.06967353  -8.84728111  -7.61024191\n",
      "  -8.84728111  -7.10004718  -7.29708402  -7.65463603  -8.65463603\n",
      "  -8.40670852  -7.61024191  -9.06967353  -9.65463603  -6.01077984\n",
      "  -8.06967353  -7.65463603 -11.65463603  -8.33270793  -9.84728111\n",
      "  -7.79665503  -9.19520441  -9.19520441 -11.06967353  -8.19520441\n",
      "  -9.06967353  -7.79665503  -6.87327632  -7.26231861  -7.33270793\n",
      "  -7.29708402 -10.33270793  -7.52535301  -9.19520441  -7.74774543\n",
      "  -7.48471103  -8.65463603  -7.10004718  -6.82174601  -8.65463603\n",
      "  -8.56717319  -7.84728111  -6.2795966   -8.33270793  -6.48471103\n",
      "  -9.06967353  -9.19520441  -7.70043972  -7.03992618  -6.87327632\n",
      "  -8.19520441  -5.91316904  -8.65463603  -6.42581734  -8.06967353\n",
      "  -7.03992618  -5.98221069  -8.95419631  -7.89974853  -7.44518266\n",
      "  -9.84728111  -6.13107407  -7.84728111 -10.06967353 -12.65463603\n",
      "  -6.89974853  -8.19520441  -8.13107407  -7.79665503  -8.33270793\n",
      "  -7.95419631  -8.95419631  -9.06967353 -12.65463603  -9.84728111\n",
      "  -8.48471103  -9.06967353  -7.44518266  -7.44518266  -6.63226822\n",
      "  -6.61024191  -8.65463603  -6.46481147  -7.61024191  -8.26231861\n",
      " -10.06967353  -8.84728111  -6.35085528  -7.84728111  -7.52535301\n",
      "  -9.48471103  -8.33270793 -11.06967353  -7.65463603  -7.26231861\n",
      "  -7.89974853  -9.19520441  -7.13107407  -9.33270793  -8.84728111\n",
      "  -7.95419631  -8.48471103  -9.19520441  -7.19520441  -6.98221069\n",
      " -12.65463603  -7.26231861  -7.56717319  -7.84728111  -7.74774543\n",
      "  -6.92671557  -8.84728111 -10.06967353  -8.19520441  -6.82174601\n",
      "  -9.06967353  -9.84728111  -8.48471103  -7.79665503  -7.89974853\n",
      "  -7.48471103  -6.84728111  -9.06967353  -7.61024191  -8.26231861\n",
      "  -7.84728111  -7.74774543  -8.74774543  -8.33270793  -7.29708402\n",
      " -10.33270793  -8.06967353  -7.74774543  -8.26231861  -6.70043972\n",
      "  -8.65463603  -6.92671557  -8.65463603  -6.72389869  -8.06967353\n",
      "  -9.06967353 -12.65463603  -7.29708402  -7.61024191  -8.13107407\n",
      "  -7.13107407  -8.56717319  -7.40670852  -8.33270793  -7.84728111\n",
      "  -7.26231861  -6.92671557  -7.48471103  -8.48471103  -8.40670852\n",
      "  -8.26231861  -8.06967353  -8.19520441  -8.65463603  -7.40670852\n",
      "  -8.65463603  -8.40670852  -7.48471103  -7.79665503  -7.95419631\n",
      "  -8.84728111 -10.33270793  -8.33270793  -7.29708402  -8.01077984\n",
      "  -9.84728111  -8.19520441  -8.84728111  -7.65463603 -10.65463603\n",
      "  -8.74774543  -7.79665503  -9.19520441  -6.79665503  -7.70043972\n",
      "  -7.74774543  -9.06967353  -9.84728111  -8.26231861 -12.65463603\n",
      " -10.33270793  -8.26231861 -10.33270793 -10.33270793  -8.56717319\n",
      " -10.06967353  -7.48471103  -8.40670852  -9.33270793  -8.95419631\n",
      "  -9.65463603  -8.74774543  -7.56717319  -8.48471103  -7.79665503\n",
      "  -8.01077984 -12.65463603  -8.01077984  -8.56717319  -7.74774543\n",
      "  -7.40670852  -9.65463603  -7.79665503  -7.79665503  -8.56717319\n",
      "  -9.84728111  -8.56717319  -8.33270793  -8.40670852 -11.06967353\n",
      "  -6.63226822  -9.65463603  -9.19520441  -7.03992618  -7.19520441\n",
      "  -8.06967353  -8.13107407  -8.56717319  -7.33270793  -9.48471103\n",
      "  -7.79665503  -7.61024191  -9.48471103 -12.65463603  -8.40670852\n",
      "  -8.40670852  -7.65463603  -6.84728111  -8.48471103  -9.19520441\n",
      "  -6.61024191  -8.13107407 -10.65463603  -8.56717319  -8.95419631\n",
      "  -9.65463603 -10.33270793  -7.56717319  -8.19520441  -8.26231861\n",
      "  -7.74774543  -8.48471103 -10.06967353  -8.40670852  -7.36923381\n",
      "  -7.36923381  -7.44518266  -8.06967353  -7.70043972  -8.06967353\n",
      "  -8.26231861  -7.74774543  -8.84728111  -8.56717319  -8.56717319\n",
      "  -8.06967353  -8.56717319  -8.65463603  -8.74774543 -11.06967353\n",
      "  -8.26231861  -8.40670852  -9.06967353  -7.95419631  -9.65463603\n",
      "  -7.89974853 -10.65463603  -8.65463603  -8.74774543  -7.16278293\n",
      "  -8.56717319  -8.95419631  -7.79665503  -7.95419631  -7.74774543\n",
      "  -8.19520441  -7.65463603  -7.22837127  -8.95419631  -8.13107407\n",
      "  -8.19520441  -7.36923381  -7.84728111  -8.06967353  -8.06967353\n",
      "  -8.06967353 -11.65463603  -7.84728111  -7.95419631 -11.06967353\n",
      "  -8.13107407  -9.33270793  -8.74774543  -9.48471103  -8.26231861\n",
      "  -8.84728111 -12.65463603  -9.19520441  -9.19520441  -9.65463603\n",
      "  -8.65463603  -7.48471103  -7.22837127  -8.26231861  -9.06967353\n",
      "  -8.74774543 -11.65463603  -8.13107407  -8.65463603 -11.06967353\n",
      "  -9.48471103  -8.74774543 -12.65463603 -12.65463603  -9.19520441\n",
      "  -8.95419631  -8.95419631]\n"
     ]
    }
   ],
   "source": [
    "# compute log priors\n",
    "from math import log2\n",
    "log_prior_republican = log2(nTrainR/len(tweets))\n",
    "log_prior_democrat = log2(nTrainD/len(tweets))\n",
    "# compute log likelihoods. First column is log likelihoods. \n",
    "log_likelihoods_R = np.zeros((927,2))\n",
    "log_likelihoods_D = np.zeros((927,2))\n",
    "\n",
    "for i in range(927):\n",
    "    log_likelihoods_R[i,0] = log2((rfmatarray[:,i].sum() + 1)/(rfmatarray.shape[0] + 2))\n",
    "    log_likelihoods_R[i,1] = log2(1- (rfmatarray[:,i].sum() + 1)/(rfmatarray.shape[0] + 2))\n",
    "    log_likelihoods_D[i,0] = log2((dfmatarray[:,i].sum() + 1)/(dfmatarray.shape[0] + 2))\n",
    "    log_likelihoods_D[i,1] = log2(1- (dfmatarray[:,i].sum() + 1)/(dfmatarray.shape[0] + 2))\n",
    "\n",
    "print('The log prior for Republican:', log_prior_republican)\n",
    "print('The log prior for Democrat:', log_prior_democrat)\n",
    "print('The log likelihoods for Republican:', log_likelihoods_R[:,0])\n",
    "print('The log likelihoods for Democrat:', log_likelihoods_D[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Set\n",
    "\n",
    "Now we have a trained Naive Bayes classifier. We will load the test data set and make the predictions. Note: If a token is not a feature, ignore it. \n",
    "\n",
    "Problem 4 (5 points) Load test data and tokenize\n",
    "\n",
    "Problem 5 (30 points) Using the trained NB classifier predict the labels\n",
    "\n",
    "Problem 6 (5 points) Calculate accuracy, recall, and precision of your predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4298\n"
     ]
    }
   ],
   "source": [
    "# Load test data and tokenize.\n",
    "tweets_test = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "all_tokens_test = tokenize_tweets2(tweets_test)\n",
    "print(len(all_tokens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a 4298*927 numpy array\n",
    "test = np.zeros((4298,927), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the features appeared in every sentences.\n",
    "for i in range(4298):\n",
    "    for j in range(len(top_words)):\n",
    "        for element in all_tokens_test[i]:\n",
    "            if element == top_words[j]:\n",
    "                test[i][j] = test[i][j]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test dataset, I only emphasize whether it's token or not token. So I initialize test_record array.\n",
    "# Number of rows is 4298. Number of solumns is 930. The first 927 columns are recording the 927 features. The last\n",
    "# three columns for posteriori probability of republican, posteriori probability of democrat, and result label.\n",
    "test_record = np.zeros((4298,930), dtype=int)\n",
    "for i in range(4298):\n",
    "    for j in range(len(top_words)):\n",
    "        if test[i][j] >=1:\n",
    "            test_record[i][j] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the posteriori probability of republican, posteriori probability of democrat to log_prior\n",
    "test_record[:,927] = log_prior_republican\n",
    "test_record[:,928] = log_prior_democrat\n",
    "\n",
    "for i in range(4298):\n",
    "    for j in range(927):\n",
    "        if test_record[i][j] == 1:\n",
    "            test_record[i][927] = test_record[i][927] + log_likelihoods_R[j][0]\n",
    "            test_record[i][928] = test_record[i][928] + log_likelihoods_D[j][0]\n",
    "        else:\n",
    "            test_record[i][927] = test_record[i][927] + log_likelihoods_R[j][1]\n",
    "            test_record[i][928] = test_record[i][928] + log_likelihoods_D[j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the posteriori probabilities and get the labels. 0 for republican. 1 for democrat\n",
    "for i in range(4298):\n",
    "    if test_record[i][927] > test_record[i][928]:\n",
    "        test_record[i][929] = 0\n",
    "    else:\n",
    "        test_record[i][929] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real labels saved in test_actual_label list\n",
    "label = tweets_test['screen_name']\n",
    "test_actual_label = []\n",
    "\n",
    "for i in range(4298):\n",
    "    if label[i] == 'TheDemocrats' or label[i] == 'HillaryClinton' or label[i] == 'timkaine':\n",
    "        test_actual_label.append(1)\n",
    "    else:\n",
    "        test_actual_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_label = test_record[:,-1]\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "for i in range(4298):\n",
    "    if test_actual_label[i] == 1 and test_predict_label[i] == 1:\n",
    "        TP = TP +1\n",
    "    elif test_actual_label[i] == 1 and test_predict_label[i] == 0:\n",
    "        FN = FN +1\n",
    "    elif test_actual_label[i] == 0 and test_predict_label[i] == 1:\n",
    "        FP = FP +1\n",
    "    else:\n",
    "        TN = TN +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8024662633783155\n",
      "Recall: 0.7896645512239348\n",
      "Precision: 0.8189938881053126\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "Recall = TP/(TP+FN)\n",
    "Precision = TP/(TP+FP)\n",
    "print(\"Accuracy: \",Accuracy)\n",
    "print(\"Recall:\", Recall)\n",
    "print(\"Precision:\", Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 (5 points) List the features with top ten likelihoods for each of the two classes. What is the likelihood for 'hillary', that is, P(hillary|class)? Is it in the top ten? How important is it in this classification problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBased on the results below, 'hillary' is in top ten likelihoods features. But it is not important in this \\nclassification problem because the difference between P(hillary|Republican) and P(hillary|Democrat) is quite small.\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Based on the results below, 'hillary' is in top ten likelihoods features. But it is not important in this \n",
    "classification problem because the difference between P(hillary|Republican) and P(hillary|Democrat) is quite small.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods_R_list = log_likelihoods_R[:,0].tolist()\n",
    "log_likelihoods_D_list = log_likelihoods_D[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.argsort(log_likelihoods_R_list)[::-1][0:10]\n",
    "b = np.argsort(log_likelihoods_D_list)[::-1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_likelihoods_R_feature = []\n",
    "top_ten_likelihoods_D_feature = []\n",
    "for i in range(10):\n",
    "    top_ten_likelihoods_R_feature.append(top_words[a[i]])\n",
    "    top_ten_likelihoods_D_feature.append(top_words[b[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features with top ten likelihoods for Republican: ['clinton', 'hillary', 'great', 'thank', 'today', 'new', 'day', 'indiana', 'job', 'state']\n",
      "The features with top ten likelihoods for Democrats: ['trump', 'hillary', 'donald', 'president', 'today', 'american', 'make', 'u', 'vote', 'one']\n"
     ]
    }
   ],
   "source": [
    "print('The features with top ten likelihoods for Republican:',top_ten_likelihoods_R_feature)\n",
    "print('The features with top ten likelihoods for Democrats:',top_ten_likelihoods_D_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index of 'hillary' in the top_words\n",
    "for i in range(927):\n",
    "    if top_words[i] == 'hillary':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log2 possibility of hillary in Republican is -3.4789277942630945\n",
      "The log2 possibility of hillary in Democrat is -3.4922446997710623\n",
      "P(hillary|Republican) 0.0896888346552776\n",
      "P(hillary|Democrat) 0.08886476426799009\n"
     ]
    }
   ],
   "source": [
    "print('The log2 possibility of hillary in Republican is',log_likelihoods_R_list[i])\n",
    "print('The log2 possibility of hillary in Democrat is', log_likelihoods_D_list[i])\n",
    "print('P(hillary|Republican)', 2**log_likelihoods_R_list[i])\n",
    "print('P(hillary|Democrat)', 2**log_likelihoods_D_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8 (5 points) How important are the priors in this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom the results below, the priors are not important in this problem because the difference is small. \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "From the results below, the priors are not important in this problem because the difference is small. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_prior_republican is -0.9880640452604194\n",
      "log_prior_democrat is -1.012035529743634\n"
     ]
    }
   ],
   "source": [
    "print(\"log_prior_republican is\", log_prior_republican)\n",
    "print(\"log_prior_democrat is\", log_prior_democrat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credit (5 points): Compute the accuracy of the test set without Laplace smoothing and compare with the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBased on the solution below, after comparing the two accuracies (below and above), I find the accuracies dropped a lot \\nfrom 0.8024662633783155 (with Laplace smoothing) to 0.5132619823173569 (without Laplace smoothing).\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Based on the solution below, after comparing the two accuracies (below and above), I find the accuracies dropped a lot \n",
    "from 0.8024662633783155 (with Laplace smoothing) to 0.5132619823173569 (without Laplace smoothing).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_republican = nTrainR/len(tweets)\n",
    "prior_democrat = nTrainD/len(tweets)\n",
    "# compute log likelihoods\n",
    "likelihoods_R = np.zeros((927,2))\n",
    "likelihoods_D = np.zeros((927,2))\n",
    "\n",
    "for i in range(927):\n",
    "    likelihoods_R[i,0] = (rfmatarray[:,i].sum() )/(rfmatarray.shape[0])\n",
    "    likelihoods_R[i,1] = 1- (rfmatarray[:,i].sum())/(rfmatarray.shape[0])\n",
    "    likelihoods_D[i,0] = (dfmatarray[:,i].sum())/(dfmatarray.shape[0])\n",
    "    likelihoods_D[i,1] = 1- (dfmatarray[:,i].sum())/(dfmatarray.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the posteriori probability of republican, posteriori probability of democrat to priors\n",
    "test_record[:,927] = prior_republican\n",
    "test_record[:,928] = prior_democrat\n",
    "\n",
    "for i in range(4298):\n",
    "    for j in range(927):\n",
    "        if test_record[i][j] == 1:\n",
    "            test_record[i][927] = test_record[i][927] * likelihoods_R[j][0]\n",
    "            test_record[i][928] = test_record[i][928] * likelihoods_D[j][0]\n",
    "        else:\n",
    "            test_record[i][927] = test_record[i][927] * likelihoods_R[j][1]\n",
    "            test_record[i][928] = test_record[i][928] * likelihoods_D[j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the posteriori probabilities and get the labels. 0 for republican. 1 for democrat\n",
    "for i in range(4298):\n",
    "    if test_record[i][927] > test_record[i][928]:\n",
    "        test_record[i][929] = 0\n",
    "    else:\n",
    "        test_record[i][929] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_label = test_record[:,-1]\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "for i in range(4298):\n",
    "    if test_actual_label[i] == 1 and test_predict_label[i] == 1:\n",
    "        TP = TP +1\n",
    "    elif test_actual_label[i] == 1 and test_predict_label[i] == 0:\n",
    "        FN = FN +1\n",
    "    elif test_actual_label[i] == 0 and test_predict_label[i] == 1:\n",
    "        FP = FP +1\n",
    "    else:\n",
    "        TN = TN +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Laplace smoothing:  0.8024662633783155\n",
      "Accuracy without Laplace smoothing:  0.5132619823173569\n"
     ]
    }
   ],
   "source": [
    "Accuracy2 = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(\"Accuracy with Laplace smoothing: \", Accuracy)\n",
    "print(\"Accuracy without Laplace smoothing: \",Accuracy2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
